{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2451b503-8b08-4cea-ae6f-c82d7899d3dd",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Implement convolution operation for a sample image of shape (H=6, W=6, C=1) with a\n",
    "random kernel of size (3,3) using torch.nn.functional.conv2d. \n",
    "\n",
    "What is the dimension of the output image? Apply, various values for parameter stride=1 and note the change in the dimension of the output image. Arrive at an equation for the output image size with respect to the kernel size and stride and verify your answer with code. Now, repeat the exercise by changing padding parameter. Obtain a formula using kernel, stride, and padding to get the output image size. What is the total number of parameters in your network? Verify with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63beb18-756e-4d29-8b0c-b9573e28f8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image= tensor([[0.9639, 0.2708, 0.3234, 0.9133, 0.3223, 0.8420],\n",
      "        [0.8119, 0.0347, 0.5326, 0.3417, 0.8358, 0.9684],\n",
      "        [0.9019, 0.5281, 0.7605, 0.0390, 0.9906, 0.3970],\n",
      "        [0.6022, 0.7848, 0.7749, 0.6226, 0.3611, 0.5193],\n",
      "        [0.7980, 0.4499, 0.7924, 0.3650, 0.7416, 0.3754],\n",
      "        [0.3243, 0.4975, 0.8319, 0.3352, 0.9380, 0.7966]])\n",
      "image.shape= torch.Size([1, 6, 6])\n",
      "image.shape= torch.Size([1, 1, 6, 6])\n",
      "image= tensor([[[[0.9639, 0.2708, 0.3234, 0.9133, 0.3223, 0.8420],\n",
      "          [0.8119, 0.0347, 0.5326, 0.3417, 0.8358, 0.9684],\n",
      "          [0.9019, 0.5281, 0.7605, 0.0390, 0.9906, 0.3970],\n",
      "          [0.6022, 0.7848, 0.7749, 0.6226, 0.3611, 0.5193],\n",
      "          [0.7980, 0.4499, 0.7924, 0.3650, 0.7416, 0.3754],\n",
      "          [0.3243, 0.4975, 0.8319, 0.3352, 0.9380, 0.7966]]]])\n",
      "kernel= tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "outimage= tensor([[[[5.1277, 3.7440, 5.0592, 5.6501],\n",
      "          [5.7316, 4.4189, 5.2589, 5.0755],\n",
      "          [6.3927, 5.1173, 5.4477, 4.4116],\n",
      "          [5.8560, 5.4543, 5.7627, 5.0547]]]])\n",
      "Dimension of output image S-1 P-0:  torch.Size([1, 1, 4, 4])\n",
      "Manually dim of output S-1 P-0:  [1, 1, 4, 4]\n",
      "Dimension of output image S-1 P-1: torch.Size([1, 1, 6, 6])\n",
      "Manually dim of output S-1 P-1:  [3, 3, 6, 6]\n",
      "Dimension of output image S-1 P-2: torch.Size([1, 1, 8, 8])\n",
      "Manually dim of output S-1 P-2:  [5, 5, 8, 8]\n",
      "Dimension of output image S-2 P-1:  torch.Size([1, 1, 3, 3])\n",
      "Manually dim of output S-2 P-1:  [2, 2, 3, 3]\n",
      "Dimension of output image S-2 P-1: torch.Size([1, 1, 2, 2])\n",
      "Manually dim of output S-3 P-1:  [1, 1, 2, 2]\n",
      "Number of Learnable Parameters = 9\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "image = torch.rand(6,6)\n",
    "print(\"image=\", image)\n",
    "#Add a new dimension along 0th dimension\n",
    "#i.e. (6,6) becomes (1,6,6). This is because\n",
    "#pytorch expects the input to conv2D as 4d tensor\n",
    "image = image.unsqueeze(dim=0)\n",
    "print(\"image.shape=\", image.shape)\n",
    "image = image.unsqueeze(dim=0)\n",
    "print(\"image.shape=\", image.shape)\n",
    "print(\"image=\", image)\n",
    "kernel = torch.ones(3,3)\n",
    "#kernel = torch.rand(3,3)\n",
    "print(\"kernel=\", kernel)\n",
    "kernel = kernel.unsqueeze(dim=0)\n",
    "kernel = kernel.unsqueeze(dim=0)\n",
    "\n",
    "def out_dim(in_shape,stride,padding,kernel_shape):\n",
    "    out_shape = [0 for i in range(4)]\n",
    "    for dim in range(len(in_shape)):\n",
    "        out_shape[dim] = (in_shape[dim] + 2*padding - kernel_shape[dim])//stride + 1\n",
    "    return out_shape\n",
    "    \n",
    "#Stride 1 Padding 0\n",
    "outimage = F.conv2d(image, kernel, stride=1, padding=0)\n",
    "print(\"outimage=\", outimage)\n",
    "print(\"Dimension of output image S-1 P-0: \",outimage.shape)\n",
    "print(\"Manually dim of output S-1 P-0: \",out_dim(image.shape,1,0,kernel.shape))\n",
    "\n",
    "#Stride 1 Padding 1\n",
    "outimage = F.conv2d(image, kernel, stride=1, padding=1)\n",
    "print(\"Dimension of output image S-1 P-1:\",outimage.shape)\n",
    "print(\"Manually dim of output S-1 P-1: \",out_dim(image.shape,1,1,kernel.shape))\n",
    "\n",
    "#Stride 1 Padding 2\n",
    "outimage = F.conv2d(image, kernel, stride=1, padding=2)\n",
    "print(\"Dimension of output image S-1 P-2:\",outimage.shape)\n",
    "print(\"Manually dim of output S-1 P-2: \",out_dim(image.shape,1,2,kernel.shape))\n",
    "\n",
    "#Stride 2 Padding 1\n",
    "outimage = F.conv2d(image, kernel, stride=2, padding=1)\n",
    "print(\"Dimension of output image S-2 P-1: \",outimage.shape)\n",
    "print(\"Manually dim of output S-2 P-1: \",out_dim(image.shape,2,1,kernel.shape))\n",
    "\n",
    "#Stride 3 Padding 1\n",
    "outimage = F.conv2d(image, kernel, stride=3, padding=1)\n",
    "print(\"Dimension of output image S-2 P-1:\",outimage.shape)\n",
    "print(\"Manually dim of output S-3 P-1: \",out_dim(image.shape,3,1,kernel.shape))\n",
    "\n",
    "print(\"Number of Learnable Parameters = 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eced408-fccf-4442-8cb5-a40b2b86f7c6",
   "metadata": {},
   "source": [
    "## Q2 \n",
    "\n",
    "Apply torch.nn.Conv2d to the input image of Qn 1 with out-channel=3 and observe the\n",
    "output. Implement the equivalent of torch.nn.Conv2d using the torch.nn.functional.conv2D\n",
    "to get the same output. You may ignore bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b074b7-75ad-4ae6-bb2d-10f72b6a15d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel parameters for 3 channels: \n",
      "Parameter containing:\n",
      "tensor([[[[-0.1506,  0.2808,  0.0680],\n",
      "          [ 0.0190,  0.1268,  0.2866],\n",
      "          [-0.1762, -0.0606,  0.1602]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1400, -0.3048, -0.1191],\n",
      "          [-0.1460,  0.2342,  0.2060],\n",
      "          [ 0.2009, -0.0864,  0.0813]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3144, -0.1243, -0.0555],\n",
      "          [-0.1734,  0.2955,  0.0847],\n",
      "          [-0.3252, -0.2590, -0.2927]]]], requires_grad=True)\n",
      "Output image using torch.nn.Conv2d: \n",
      "tensor([[[[ 0.3373,  0.3251,  0.1675,  0.3529],\n",
      "          [ 0.3324,  0.2960,  0.4665,  0.3945],\n",
      "          [ 0.1618,  0.5531,  0.4188,  0.3958],\n",
      "          [ 0.1108,  0.2211,  0.2117,  0.2190]],\n",
      "\n",
      "         [[-0.0404,  0.0563,  0.3575,  0.1941],\n",
      "          [ 0.1107,  0.4371, -0.0794,  0.2294],\n",
      "          [ 0.1833, -0.0610,  0.0713,  0.1953],\n",
      "          [ 0.0381,  0.0650,  0.0443,  0.1117]],\n",
      "\n",
      "         [[-0.5723, -0.3482, -0.2202, -0.5652],\n",
      "          [-0.3122, -0.0678, -0.5608, -0.4437],\n",
      "          [ 0.0595, -0.1923,  0.0496, -0.0366],\n",
      "          [-0.6073, -0.6483, -0.4349, -0.4466]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "Output image using torch.nn.functional.conv2d: \n",
      "tensor([[[[ 0.3373,  0.3251,  0.1675,  0.3529],\n",
      "          [ 0.3324,  0.2960,  0.4665,  0.3945],\n",
      "          [ 0.1618,  0.5531,  0.4188,  0.3958],\n",
      "          [ 0.1108,  0.2211,  0.2117,  0.2190]],\n",
      "\n",
      "         [[-0.0404,  0.0563,  0.3575,  0.1941],\n",
      "          [ 0.1107,  0.4371, -0.0794,  0.2294],\n",
      "          [ 0.1833, -0.0610,  0.0713,  0.1953],\n",
      "          [ 0.0381,  0.0650,  0.0443,  0.1117]],\n",
      "\n",
      "         [[-0.5723, -0.3482, -0.2202, -0.5652],\n",
      "          [-0.3122, -0.0678, -0.5608, -0.4437],\n",
      "          [ 0.0595, -0.1923,  0.0496, -0.0366],\n",
      "          [-0.6073, -0.6483, -0.4349, -0.4466]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "image= torch.tensor([[[[0.2557, 0.9236, 0.4913, 0.3200, 0.4958, 0.2214],\n",
    "          [0.7554, 0.6501, 0.0107, 0.8675, 0.5163, 0.6102],\n",
    "          [0.8228, 0.1919, 0.8724, 0.8043, 0.3882, 0.9689],\n",
    "          [0.4894, 0.5116, 0.5624, 0.6949, 0.6289, 0.9802],\n",
    "          [0.3913, 0.2773, 0.1427, 0.3717, 0.4154, 0.3669],\n",
    "          [0.8327, 0.8157, 0.7192, 0.9387, 0.4569, 0.6776]]]])\n",
    "\n",
    "conv = nn.Conv2d(in_channels=1,out_channels=3,kernel_size=3,stride=1,padding=0,bias=False)\n",
    "print(\"Kernel parameters for 3 channels: \")\n",
    "kernel = conv.weight\n",
    "print(conv.weight)\n",
    "print(\"Output image using torch.nn.Conv2d: \")\n",
    "out_image = print(conv(image))\n",
    "\n",
    "import torch.nn.functional as F\n",
    "out_image = F.conv2d(image,kernel,stride=1,padding=0)\n",
    "print(\"Output image using torch.nn.functional.conv2d: \")\n",
    "print(out_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e426921-6471-48ed-b498-9017accd3932",
   "metadata": {},
   "source": [
    "## Q3 \n",
    "\n",
    "Implement CNN for classifying digits in MNIST dataset using PyTorch. Display the classification accuracy in the form of a Confusion matrix. Verify the number of learnable parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe4dd0e-3792-417b-ab9c-c356feefb3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.308\n",
      "[1,   200] loss: 2.300\n",
      "[1,   300] loss: 2.290\n",
      "[1,   400] loss: 2.278\n",
      "[1,   500] loss: 2.260\n",
      "[1,   600] loss: 2.227\n",
      "[1,   700] loss: 2.166\n",
      "[1,   800] loss: 2.036\n",
      "[1,   900] loss: 1.802\n",
      "[1,  1000] loss: 1.477\n",
      "[1,  1100] loss: 1.195\n",
      "[1,  1200] loss: 0.966\n",
      "[2,   100] loss: 0.783\n",
      "[2,   200] loss: 0.689\n",
      "[2,   300] loss: 0.624\n",
      "[2,   400] loss: 0.545\n",
      "[2,   500] loss: 0.498\n",
      "[2,   600] loss: 0.452\n",
      "[2,   700] loss: 0.405\n",
      "[2,   800] loss: 0.361\n",
      "[2,   900] loss: 0.356\n",
      "[2,  1000] loss: 0.344\n",
      "[2,  1100] loss: 0.308\n",
      "[2,  1200] loss: 0.290\n",
      "[3,   100] loss: 0.285\n",
      "[3,   200] loss: 0.279\n",
      "[3,   300] loss: 0.269\n",
      "[3,   400] loss: 0.247\n",
      "[3,   500] loss: 0.224\n",
      "[3,   600] loss: 0.217\n",
      "[3,   700] loss: 0.227\n",
      "[3,   800] loss: 0.212\n",
      "[3,   900] loss: 0.215\n",
      "[3,  1000] loss: 0.194\n",
      "[3,  1100] loss: 0.217\n",
      "[3,  1200] loss: 0.209\n",
      "[4,   100] loss: 0.186\n",
      "[4,   200] loss: 0.188\n",
      "[4,   300] loss: 0.178\n",
      "[4,   400] loss: 0.188\n",
      "[4,   500] loss: 0.178\n",
      "[4,   600] loss: 0.175\n",
      "[4,   700] loss: 0.172\n",
      "[4,   800] loss: 0.167\n",
      "[4,   900] loss: 0.152\n",
      "[4,  1000] loss: 0.168\n",
      "[4,  1100] loss: 0.176\n",
      "[4,  1200] loss: 0.157\n",
      "[5,   100] loss: 0.173\n",
      "[5,   200] loss: 0.160\n",
      "[5,   300] loss: 0.132\n",
      "[5,   400] loss: 0.135\n",
      "[5,   500] loss: 0.144\n",
      "[5,   600] loss: 0.143\n",
      "[5,   700] loss: 0.133\n",
      "[5,   800] loss: 0.129\n",
      "[5,   900] loss: 0.144\n",
      "[5,  1000] loss: 0.135\n",
      "[5,  1100] loss: 0.124\n",
      "[5,  1200] loss: 0.139\n",
      "[6,   100] loss: 0.126\n",
      "[6,   200] loss: 0.139\n",
      "[6,   300] loss: 0.135\n",
      "[6,   400] loss: 0.134\n",
      "[6,   500] loss: 0.121\n",
      "[6,   600] loss: 0.113\n",
      "[6,   700] loss: 0.132\n",
      "[6,   800] loss: 0.115\n",
      "[6,   900] loss: 0.113\n",
      "[6,  1000] loss: 0.119\n",
      "[6,  1100] loss: 0.109\n",
      "[6,  1200] loss: 0.113\n",
      "Finished Training. Final loss = 0.0790271908044815, Total params = 149798\n",
      "Correct = 9693, Total = 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,64,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(64,128,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(128,64,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(64,20,bias=True),\n",
    "                                                 nn.ReLU(),\n",
    "                                                 nn.Linear(20,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root=\"./data\",download = True,train=True,transform=ToTensor())\n",
    "train_loader = DataLoader(mnist_trainset,batch_size=50,shuffle=True)\n",
    "mnist_testset = datasets.MNIST(root=\"./data\",download = True,train=False,transform=ToTensor())\n",
    "test_loader = DataLoader(mnist_testset,batch_size=50,shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNClassifier().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "\n",
    "total_params = 0\n",
    "for name,param in model.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")\n",
    "\n",
    "correct,total = 0,0\n",
    "for i,vdata in enumerate(test_loader):\n",
    "    tinputs,tlabels = vdata[0].to(device), vdata[1].to(device)\n",
    "    toutputs = model(tinputs)\n",
    "\n",
    "    _,predicted = torch.max(toutputs,1)\n",
    "    total += tlabels.size(0)\n",
    "    correct += (predicted==tlabels).sum()\n",
    "        \n",
    "print(f\"Correct = {correct}, Total = {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5a7d9-ed64-4d1d-ab72-ed8c580a6cc9",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "Modify CNN of Qn. 3 to reduce the number of parameters in the network. Draw a plot of\n",
    "percentage drop in parameters vs accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c50ee7e0-4821-4662-ad21-4f671aa6744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.300\n",
      "[1,   200] loss: 2.291\n",
      "[1,   300] loss: 2.271\n",
      "[1,   400] loss: 2.249\n",
      "[1,   500] loss: 2.199\n",
      "[1,   600] loss: 2.108\n",
      "[1,   700] loss: 1.883\n",
      "[1,   800] loss: 1.527\n",
      "[1,   900] loss: 1.202\n",
      "[1,  1000] loss: 0.996\n",
      "[1,  1100] loss: 0.855\n",
      "[1,  1200] loss: 0.772\n",
      "[2,   100] loss: 0.689\n",
      "[2,   200] loss: 0.608\n",
      "[2,   300] loss: 0.569\n",
      "[2,   400] loss: 0.530\n",
      "[2,   500] loss: 0.520\n",
      "[2,   600] loss: 0.456\n",
      "[2,   700] loss: 0.423\n",
      "[2,   800] loss: 0.379\n",
      "[2,   900] loss: 0.405\n",
      "[2,  1000] loss: 0.388\n",
      "[2,  1100] loss: 0.388\n",
      "[2,  1200] loss: 0.307\n",
      "[3,   100] loss: 0.314\n",
      "[3,   200] loss: 0.336\n",
      "[3,   300] loss: 0.323\n",
      "[3,   400] loss: 0.345\n",
      "[3,   500] loss: 0.292\n",
      "[3,   600] loss: 0.284\n",
      "[3,   700] loss: 0.276\n",
      "[3,   800] loss: 0.254\n",
      "[3,   900] loss: 0.283\n",
      "[3,  1000] loss: 0.278\n",
      "[3,  1100] loss: 0.267\n",
      "[3,  1200] loss: 0.258\n",
      "[4,   100] loss: 0.262\n",
      "[4,   200] loss: 0.236\n",
      "[4,   300] loss: 0.232\n",
      "[4,   400] loss: 0.242\n",
      "[4,   500] loss: 0.228\n",
      "[4,   600] loss: 0.231\n",
      "[4,   700] loss: 0.225\n",
      "[4,   800] loss: 0.250\n",
      "[4,   900] loss: 0.235\n",
      "[4,  1000] loss: 0.234\n",
      "[4,  1100] loss: 0.219\n",
      "[4,  1200] loss: 0.220\n",
      "[5,   100] loss: 0.209\n",
      "[5,   200] loss: 0.208\n",
      "[5,   300] loss: 0.194\n",
      "[5,   400] loss: 0.213\n",
      "[5,   500] loss: 0.208\n",
      "[5,   600] loss: 0.209\n",
      "[5,   700] loss: 0.196\n",
      "[5,   800] loss: 0.201\n",
      "[5,   900] loss: 0.199\n",
      "[5,  1000] loss: 0.169\n",
      "[5,  1100] loss: 0.183\n",
      "[5,  1200] loss: 0.195\n",
      "[6,   100] loss: 0.183\n",
      "[6,   200] loss: 0.167\n",
      "[6,   300] loss: 0.189\n",
      "[6,   400] loss: 0.174\n",
      "[6,   500] loss: 0.183\n",
      "[6,   600] loss: 0.180\n",
      "[6,   700] loss: 0.169\n",
      "[6,   800] loss: 0.185\n",
      "[6,   900] loss: 0.157\n",
      "[6,  1000] loss: 0.153\n",
      "[6,  1100] loss: 0.172\n",
      "[6,  1200] loss: 0.177\n",
      "Finished Training. Final loss = 0.06298686563968658, Total params = 9594\n",
      "Correct = 9573, Total = 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class CNNClassifier1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,16,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(16,32,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(32,16,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(16,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root=\"./data\",download = True,train=True,transform=ToTensor())\n",
    "train_loader = DataLoader(mnist_trainset,batch_size=50,shuffle=True)\n",
    "mnist_testset = datasets.MNIST(root=\"./data\",download = True,train=False,transform=ToTensor())\n",
    "test_loader = DataLoader(mnist_testset,batch_size=50,shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1 = CNNClassifier1().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model1.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "\n",
    "total_params = 0\n",
    "for name,param in model1.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model1(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")\n",
    "\n",
    "correct,total = 0,0\n",
    "for i,vdata in enumerate(test_loader):\n",
    "    tinputs,tlabels = vdata[0].to(device), vdata[1].to(device)\n",
    "    toutputs = model1(tinputs)\n",
    "\n",
    "    _,predicted = torch.max(toutputs,1)\n",
    "    total += tlabels.size(0)\n",
    "    correct += (predicted==tlabels).sum()\n",
    "        \n",
    "print(f\"Correct = {correct}, Total = {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c000d6-9bbc-4796-bf88-f31189c20dc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.304\n",
      "[1,   200] loss: 2.297\n",
      "[1,   300] loss: 2.296\n",
      "[1,   400] loss: 2.290\n",
      "[1,   500] loss: 2.282\n",
      "[1,   600] loss: 2.273\n",
      "[1,   700] loss: 2.249\n",
      "[1,   800] loss: 2.220\n",
      "[1,   900] loss: 2.163\n",
      "[1,  1000] loss: 2.068\n",
      "[1,  1100] loss: 1.913\n",
      "[1,  1200] loss: 1.639\n",
      "[2,   100] loss: 1.375\n",
      "[2,   200] loss: 1.124\n",
      "[2,   300] loss: 0.903\n",
      "[2,   400] loss: 0.767\n",
      "[2,   500] loss: 0.627\n",
      "[2,   600] loss: 0.551\n",
      "[2,   700] loss: 0.485\n",
      "[2,   800] loss: 0.399\n",
      "[2,   900] loss: 0.377\n",
      "[2,  1000] loss: 0.393\n",
      "[2,  1100] loss: 0.337\n",
      "[2,  1200] loss: 0.309\n",
      "[3,   100] loss: 0.324\n",
      "[3,   200] loss: 0.311\n",
      "[3,   300] loss: 0.257\n",
      "[3,   400] loss: 0.251\n",
      "[3,   500] loss: 0.262\n",
      "[3,   600] loss: 0.238\n",
      "[3,   700] loss: 0.236\n",
      "[3,   800] loss: 0.237\n",
      "[3,   900] loss: 0.227\n",
      "[3,  1000] loss: 0.199\n",
      "[3,  1100] loss: 0.215\n",
      "[3,  1200] loss: 0.199\n",
      "[4,   100] loss: 0.195\n",
      "[4,   200] loss: 0.197\n",
      "[4,   300] loss: 0.183\n",
      "[4,   400] loss: 0.184\n",
      "[4,   500] loss: 0.178\n",
      "[4,   600] loss: 0.179\n",
      "[4,   700] loss: 0.160\n",
      "[4,   800] loss: 0.167\n",
      "[4,   900] loss: 0.174\n",
      "[4,  1000] loss: 0.145\n",
      "[4,  1100] loss: 0.157\n",
      "[4,  1200] loss: 0.147\n",
      "[5,   100] loss: 0.145\n",
      "[5,   200] loss: 0.140\n",
      "[5,   300] loss: 0.142\n",
      "[5,   400] loss: 0.152\n",
      "[5,   500] loss: 0.137\n",
      "[5,   600] loss: 0.134\n",
      "[5,   700] loss: 0.131\n",
      "[5,   800] loss: 0.134\n",
      "[5,   900] loss: 0.120\n",
      "[5,  1000] loss: 0.133\n",
      "[5,  1100] loss: 0.122\n",
      "[5,  1200] loss: 0.131\n",
      "[6,   100] loss: 0.125\n",
      "[6,   200] loss: 0.111\n",
      "[6,   300] loss: 0.104\n",
      "[6,   400] loss: 0.121\n",
      "[6,   500] loss: 0.113\n",
      "[6,   600] loss: 0.107\n",
      "[6,   700] loss: 0.107\n",
      "[6,   800] loss: 0.123\n",
      "[6,   900] loss: 0.109\n",
      "[6,  1000] loss: 0.103\n",
      "[6,  1100] loss: 0.106\n",
      "[6,  1200] loss: 0.114\n",
      "Finished Training. Final loss = 0.04654674977064133, Total params = 601254\n",
      "[1,   100] loss: 2.307\n",
      "[1,   200] loss: 2.305\n",
      "[1,   300] loss: 2.302\n",
      "[1,   400] loss: 2.297\n",
      "[1,   500] loss: 2.296\n",
      "[1,   600] loss: 2.294\n",
      "[1,   700] loss: 2.288\n",
      "[1,   800] loss: 2.285\n",
      "[1,   900] loss: 2.276\n",
      "[1,  1000] loss: 2.262\n",
      "[1,  1100] loss: 2.240\n",
      "[1,  1200] loss: 2.200\n",
      "[2,   100] loss: 2.132\n",
      "[2,   200] loss: 1.979\n",
      "[2,   300] loss: 1.669\n",
      "[2,   400] loss: 1.324\n",
      "[2,   500] loss: 1.128\n",
      "[2,   600] loss: 0.972\n",
      "[2,   700] loss: 0.827\n",
      "[2,   800] loss: 0.737\n",
      "[2,   900] loss: 0.646\n",
      "[2,  1000] loss: 0.584\n",
      "[2,  1100] loss: 0.567\n",
      "[2,  1200] loss: 0.515\n",
      "[3,   100] loss: 0.464\n",
      "[3,   200] loss: 0.442\n",
      "[3,   300] loss: 0.400\n",
      "[3,   400] loss: 0.395\n",
      "[3,   500] loss: 0.371\n",
      "[3,   600] loss: 0.357\n",
      "[3,   700] loss: 0.330\n",
      "[3,   800] loss: 0.341\n",
      "[3,   900] loss: 0.328\n",
      "[3,  1000] loss: 0.299\n",
      "[3,  1100] loss: 0.287\n",
      "[3,  1200] loss: 0.291\n",
      "[4,   100] loss: 0.275\n",
      "[4,   200] loss: 0.281\n",
      "[4,   300] loss: 0.275\n",
      "[4,   400] loss: 0.260\n",
      "[4,   500] loss: 0.249\n",
      "[4,   600] loss: 0.244\n",
      "[4,   700] loss: 0.255\n",
      "[4,   800] loss: 0.240\n",
      "[4,   900] loss: 0.220\n",
      "[4,  1000] loss: 0.223\n",
      "[4,  1100] loss: 0.218\n",
      "[4,  1200] loss: 0.214\n",
      "[5,   100] loss: 0.225\n",
      "[5,   200] loss: 0.217\n",
      "[5,   300] loss: 0.207\n",
      "[5,   400] loss: 0.205\n",
      "[5,   500] loss: 0.201\n",
      "[5,   600] loss: 0.211\n",
      "[5,   700] loss: 0.193\n",
      "[5,   800] loss: 0.183\n",
      "[5,   900] loss: 0.192\n",
      "[5,  1000] loss: 0.182\n",
      "[5,  1100] loss: 0.192\n",
      "[5,  1200] loss: 0.169\n",
      "[6,   100] loss: 0.179\n",
      "[6,   200] loss: 0.164\n",
      "[6,   300] loss: 0.183\n",
      "[6,   400] loss: 0.173\n",
      "[6,   500] loss: 0.168\n",
      "[6,   600] loss: 0.164\n",
      "[6,   700] loss: 0.168\n",
      "[6,   800] loss: 0.174\n",
      "[6,   900] loss: 0.170\n",
      "[6,  1000] loss: 0.160\n",
      "[6,  1100] loss: 0.153\n",
      "[6,  1200] loss: 0.152\n",
      "Finished Training. Final loss = 0.18546083569526672, Total params = 38150\n",
      "[1,   100] loss: 2.303\n",
      "[1,   200] loss: 2.297\n",
      "[1,   300] loss: 2.291\n",
      "[1,   400] loss: 2.283\n",
      "[1,   500] loss: 2.271\n",
      "[1,   600] loss: 2.252\n",
      "[1,   700] loss: 2.221\n",
      "[1,   800] loss: 2.148\n",
      "[1,   900] loss: 1.972\n",
      "[1,  1000] loss: 1.600\n",
      "[1,  1100] loss: 1.213\n",
      "[1,  1200] loss: 0.978\n",
      "[2,   100] loss: 0.830\n",
      "[2,   200] loss: 0.718\n",
      "[2,   300] loss: 0.621\n",
      "[2,   400] loss: 0.587\n",
      "[2,   500] loss: 0.518\n",
      "[2,   600] loss: 0.509\n",
      "[2,   700] loss: 0.450\n",
      "[2,   800] loss: 0.448\n",
      "[2,   900] loss: 0.412\n",
      "[2,  1000] loss: 0.412\n",
      "[2,  1100] loss: 0.383\n",
      "[2,  1200] loss: 0.376\n",
      "[3,   100] loss: 0.350\n",
      "[3,   200] loss: 0.349\n",
      "[3,   300] loss: 0.314\n",
      "[3,   400] loss: 0.333\n",
      "[3,   500] loss: 0.316\n",
      "[3,   600] loss: 0.318\n",
      "[3,   700] loss: 0.289\n",
      "[3,   800] loss: 0.286\n",
      "[3,   900] loss: 0.288\n",
      "[3,  1000] loss: 0.274\n",
      "[3,  1100] loss: 0.270\n",
      "[3,  1200] loss: 0.256\n",
      "[4,   100] loss: 0.251\n",
      "[4,   200] loss: 0.264\n",
      "[4,   300] loss: 0.234\n",
      "[4,   400] loss: 0.256\n",
      "[4,   500] loss: 0.233\n",
      "[4,   600] loss: 0.229\n",
      "[4,   700] loss: 0.228\n",
      "[4,   800] loss: 0.219\n",
      "[4,   900] loss: 0.216\n",
      "[4,  1000] loss: 0.221\n",
      "[4,  1100] loss: 0.217\n",
      "[4,  1200] loss: 0.222\n",
      "[5,   100] loss: 0.200\n",
      "[5,   200] loss: 0.207\n",
      "[5,   300] loss: 0.209\n",
      "[5,   400] loss: 0.200\n",
      "[5,   500] loss: 0.200\n",
      "[5,   600] loss: 0.194\n",
      "[5,   700] loss: 0.179\n",
      "[5,   800] loss: 0.196\n",
      "[5,   900] loss: 0.190\n",
      "[5,  1000] loss: 0.170\n",
      "[5,  1100] loss: 0.173\n",
      "[5,  1200] loss: 0.174\n",
      "[6,   100] loss: 0.170\n",
      "[6,   200] loss: 0.170\n",
      "[6,   300] loss: 0.173\n",
      "[6,   400] loss: 0.158\n",
      "[6,   500] loss: 0.145\n",
      "[6,   600] loss: 0.159\n",
      "[6,   700] loss: 0.178\n",
      "[6,   800] loss: 0.161\n",
      "[6,   900] loss: 0.165\n",
      "[6,  1000] loss: 0.150\n",
      "[6,  1100] loss: 0.165\n",
      "[6,  1200] loss: 0.175\n",
      "Finished Training. Final loss = 0.13743384182453156, Total params = 9594\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root=\"./data\",download = True,train=True,transform=ToTensor())\n",
    "train_loader = DataLoader(mnist_trainset,batch_size=50,shuffle=True)\n",
    "mnist_testset = datasets.MNIST(root=\"./data\",download = True,train=False,transform=ToTensor())\n",
    "test_loader = DataLoader(mnist_testset,batch_size=50,shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "class CNNClassifier1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,128,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(128,256,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(256,128,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(128,64,bias=True),\n",
    "                                                 nn.ReLU(),\n",
    "                                                 nn.Linear(64,20,bias=True),\n",
    "                                                 nn.ReLU(),\n",
    "                                                 nn.Linear(20,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "\n",
    "class CNNClassifier2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,32,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(32,64,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(64,32,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(32,20,bias=True),\n",
    "                                                 nn.ReLU(),\n",
    "                                                 nn.Linear(20,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "    \n",
    "class CNNClassifier3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,16,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(16,32,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(32,16,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(16,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "\n",
    "model1 = CNNClassifier1().to(device)\n",
    "model2 = CNNClassifier2().to(device)\n",
    "model3 = CNNClassifier3().to(device)\n",
    "optimizer = optim.SGD(model1.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "loss = None\n",
    "total_params = 0\n",
    "\n",
    "for name,param in model1.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model1(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")\n",
    "\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "loss = None\n",
    "total_params = 0\n",
    "\n",
    "for name,param in model2.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model2(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")\n",
    "\n",
    "\n",
    "loss = None\n",
    "optimizer = optim.SGD(model3.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "\n",
    "total_params = 0\n",
    "for name,param in model3.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model3(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfeb891a-6f7a-4bf5-b9e2-5898f7f63ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f015afff10>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = [0.03902818262577057,0.08542836457490921,0.09705353528261185,0.06044217571616173]\n",
    "params = [601254,149798,38150,9594]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(params,losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0587c0-b4d4-476a-8b7c-d9e6a528346a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
